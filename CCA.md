典型相关分析（Canonical Correlation Analysis）是研究两组变量之间相关关系的一种多元统计方法，它能够评估两组或多组变量之间的内在相关性。

在一元统计分析中，用相关系数来衡量两个随机变量的线性相关关系，用[复相关系数](https://baike.baidu.com/item/%E5%A4%8D%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/1592780)研究一个随机变量与多个随机变量的线性相关关系。然而，这些方法均无法用于研究两组变量之间的相关关系，于是提出了CCA。
CCA的基本思想和PCA主成分分析有相似之处：

首先，在每组变量中寻找出变量的线性组合，使得两组的线性组合之间具有最大的相关系数；

然后选取和已经挑选出的这对线性组合不相关的另一对线性组合，并使其相关系数最大，如此下去，直到两组变量的相关性被提取完毕为止。

被选出的线性组合配对称为典型变量，它们的相关系数称为典型相关系数。



在数理统计里面，都知道相关系数这个概念。假设有两组一维的数据集X和Y，则相关系数ρ的定义为:

其中cov(X,Y)是X和Y的协方差，而D(X),D(Y)分别是X和Y的方差。相关系数ρ的取值为[-1,1],　ρ的绝对值越接近于1，则X和Y的线性相关性越高。越接近于0，则X和Y的线性相关性越低。

虽然相关系数可以很好的帮我们分析一维数据的相关性，但是对于高维数据就不能直接使用了。如上所述，如果X是包括人身高和体重两个维度的数据，而Y是包括跑步能力和跳远能力两个维度的数据，就不能直接使用相关系数的方法。那我们能不能变通一下呢？CCA给了我们变通的方法。

CCA使用的方法是将多维的X和Y都用线性变换为1维的X'和Y'，然后再使用相关系数来看X'和Y'的相关性。将数据从多维变到1位，也可以理解为CCA是在进行降维，将高维数据降到1维，然后再用相关系数进行相关性的分析。

上面提到CCA是将高维的两组数据分别降维到1维，然后用相关系数分析相关性。

但是有一个问题是，降维的标准是如何选择的呢？

>主成分分析PCA，降维的原则是投影方差最大；
>
>线性判别分析LDA，降维的原则是同类的投影方差小，异类间的投影方差大。
>
>CCA选择的投影标准是降维到1维后，两组数据的相关系数最大。

假设数据集是X和Y，X为n1×m的样本矩阵，Y为n2×m的样本矩阵。

其中m为样本个数，而n1,n2分别为X和Y的特征维度。

对于X矩阵，将其投影到1维，对应的投影向量为a, 对于Y矩阵，将其投影到1维，对应的投影向量为b, 这样X ,Y投影后得到的一维向量分别为X',Y'。

$$X'=a^TX, Y'=b^TY$$

CCA的优化目标是最大化相对系数$ρ(X′,Y′)$，得到对应的投影向量a,b（以上三值为输出），即
![image](https://user-images.githubusercontent.com/49140300/191026805-28f069be-1065-4852-9f1e-719c57c20377.png)
在投影前，一般会把原始数据进行标准化，得到均值为0而方差为1的数据X和Y。这样我们有：
![image](https://user-images.githubusercontent.com/49140300/191026833-ddaa01e4-cc6a-4397-9c3d-02511e817ba4.png)

由于X，Y的均值均为0，则![image](https://user-images.githubusercontent.com/49140300/191026937-942e3fcc-919c-473a-b73b-e14aa56d8a19.png)

令SXY=cov(X,Y)，则优化目标可以转化为：
![image](https://user-images.githubusercontent.com/49140300/191027057-54561fc3-bc26-4dab-bd83-20448c18d702.png)

由于分子分母增大相同的倍数，优化目标结果不变，我们可以采用和SVM类似的优化方法，固定分母，优化分子，具体的转化为：
![image](https://user-images.githubusercontent.com/49140300/191027119-216038f8-4726-4f93-ae76-114dffb88bf5.png)



进而CCA算法的目标最终转化为一个凸优化过程，只要求出了这个优化目标的最大值，就是前面提到的多维X和Y的相关性度量，而对应的a,b则为降维时的投影向量。

[典型相关性分析(CCA)详解](https://www.ngui.cc/el/519968.html?action=onClick)


![image](https://user-images.githubusercontent.com/49140300/191027536-6021da26-b72a-43f3-bcb5-840f40d37c43.png)
[奇异值SVD分解](https://zhuanlan.zhihu.com/p/448767610)
[特征值](https://www.zhihu.com/question/21874816/answer/181864044)

(典型相关分析（CCA）简述)[https://blog.csdn.net/iceberg7012/article/details/109189003]
(基于典型相关性分析(CCA)的多视图学习方法综述)[https://blog.csdn.net/xq151750111/article/details/123426585]
